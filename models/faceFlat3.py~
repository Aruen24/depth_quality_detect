
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import tensorflow.contrib.slim as slim
from tensorflow.python.framework import ops
from tensorflow.python.ops import control_flow_ops
#Author: huangtao




def inference(images, keep_probability=1.0, phase_train=True,
               weight_decay=0.0, reuse=None,mode="train"):
    # batch_norm_params = {
    #     # Decay for the moving averages.
    #     'decay': 0.995,
    #     # epsilon to prevent 0s in variance.
    #     'epsilon': 0.001,
    #     # force in-place updates of mean and variance estimates
    #     'updates_collections': None,
    #     # Moving averages ends up in the trainable variables collection
    #     'variables_collections': [tf.GraphKeys.TRAINABLE_VARIABLES],
    # }
    batch_norm_params = {
        'scale': True
    }

    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.separable_conv2d],
                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),
                        weights_regularizer=slim.l2_regularizer(weight_decay),
                        normalizer_fn=None,
                        normalizer_params=None):
        return inception_resnet_v1_mobilenetV2(images, is_training=phase_train,
                                   dropout_keep_prob=keep_probability,
                                   reuse=reuse, mode=mode)


def inception_resnet_v1_mobilenetV2(inputs, is_training=True,
                        dropout_keep_prob=0.8,
                        reuse=None,
                        scope='mobilenet',mode="train"):

    with tf.variable_scope(scope, 'mobilenet', [inputs], reuse=reuse):
        with slim.arg_scope([slim.batch_norm, slim.dropout],
                            is_training=is_training):
            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                                stride=1, padding='SAME'):
                # with  tf.device('/gpu:0'):
                net = slim.conv2d(inputs, 8, 3, stride=2, activation_fn=tf.nn.relu,  padding='SAME',scope='Conv1')  #56X48
                net = slim.conv2d(net, 16, 3, stride=2, activation_fn=tf.nn.relu, padding='SAME',
                                  scope='Conv2')  # 28X24
                net = slim.conv2d(net, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='SAME',
                                  scope='Conv3')  # 14X12
                # if mode=="train":
                #     net = slim.dropout(net, dropout_keep_prob, is_training=is_training)
                net = slim.conv2d(net, 32, 3, stride=2, activation_fn=tf.nn.relu, padding='SAME',
                                  scope='Conv4')  # 7X6
                if mode=="train":
                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training)

                net = slim.separable_conv2d(net, None, net.get_shape()[1:3], depth_multiplier=1, stride=1,
                                            activation_fn=tf.nn.relu,
                                            padding='VALID', scope='average')

                net = slim.flatten(net)
                if mode == "train":
                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training)

                logits = slim.fully_connected(net, 2,
                                              activation_fn=None,
                                              normalizer_fn=None,
                                              scope='logits')
                #Predictions = slim.softmax(logits)
                #Predictions = tf.identity(logits, name='out')
    return logits#,Predictions
